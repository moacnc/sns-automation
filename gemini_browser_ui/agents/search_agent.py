#!/usr/bin/env python3
"""
Search Agent - Google Search Grounding Specialist

Uses Google's official Search Grounding API to perform web searches
without bot detection issues. Returns summarized results with source URLs.
"""

import os
from typing import Optional, Dict, Any, List, Callable
from loguru import logger
from google import genai
from google.genai import types


class SearchAgent:
    """
    Search Agent using Google Search Grounding

    This agent specializes in web searches using Google's official API,
    avoiding bot detection issues that plague direct browser-based searches.
    """

    def __init__(self, api_key: Optional[str] = None, progress_callback: Optional[Callable] = None):
        """
        Initialize Search Agent

        Args:
            api_key: Google AI API key (or uses GEMINI_API_KEY env var)
            progress_callback: Optional callback for progress updates
        """
        self.api_key = api_key or os.getenv("GEMINI_API_KEY")
        if not self.api_key:
            raise ValueError("GEMINI_API_KEY not found in environment")

        # Initialize Gemini client
        self.client = genai.Client(api_key=self.api_key)

        # Model for search grounding
        self.model_name = "gemini-2.0-flash-exp"

        # Google Search Grounding tool
        self.grounding_tool = types.Tool(
            google_search=types.GoogleSearch()
        )

        # Progress callback
        self.progress_callback = progress_callback

        logger.info(f"ğŸ” SearchAgent initialized with model: {self.model_name}")

    def search(self, query: str, language: str = "ko") -> Dict[str, Any]:
        """
        Perform Google Search using Search Grounding

        Args:
            query: Search query
            language: Language for search results (default: Korean)

        Returns:
            Dictionary containing:
            - query: Original search query
            - summary: AI-generated summary of results
            - sources: List of source URLs with titles
            - search_queries: Actual search queries used by Gemini
            - grounding_metadata: Full grounding metadata
        """
        try:
            logger.info(f"ğŸ” Searching: {query}")

            if self.progress_callback:
                self.progress_callback({
                    'agent': 'search',
                    'type': 'info',
                    'message': f'ğŸ” Google Search ì‹œì‘: "{query}"'
                })

            # Build search prompt optimized for Korean/English
            search_prompt = self._build_search_prompt(query, language)

            # Configure with Google Search Grounding
            config = types.GenerateContentConfig(
                tools=[self.grounding_tool],
                temperature=0.3,  # Lower temperature for factual searches
            )

            if self.progress_callback:
                self.progress_callback({
                    'agent': 'search',
                    'type': 'info',
                    'message': 'â³ Google Search Grounding API í˜¸ì¶œ ì¤‘...'
                })

            # Call Gemini with Search Grounding
            response = self.client.models.generate_content(
                model=self.model_name,
                contents=search_prompt,
                config=config
            )

            # Extract grounding metadata
            grounding_metadata = response.candidates[0].grounding_metadata if hasattr(response, 'candidates') else None

            # Extract search queries used
            search_queries = []
            if grounding_metadata and hasattr(grounding_metadata, 'web_search_queries'):
                search_queries = grounding_metadata.web_search_queries
                logger.info(f"ğŸ“‹ Search queries used: {search_queries}")

            # Extract sources
            sources = self._extract_sources(grounding_metadata)

            # Get summary text
            summary = response.text if hasattr(response, 'text') else "ê²€ìƒ‰ ê²°ê³¼ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

            logger.info(f"âœ… Search completed: {len(sources)} sources found")

            if self.progress_callback:
                self.progress_callback({
                    'agent': 'search',
                    'type': 'info',
                    'message': f'âœ… ê²€ìƒ‰ ì™„ë£Œ: {len(sources)}ê°œ ì†ŒìŠ¤ ë°œê²¬'
                })

                # Send summary as gemini_text
                self.progress_callback({
                    'agent': 'search',
                    'type': 'gemini_text',
                    'message': f'ğŸ’­ ê²€ìƒ‰ ê²°ê³¼ ìš”ì•½:\n{summary[:500]}...'
                })

            result = {
                'query': query,
                'summary': summary,
                'sources': sources,
                'search_queries': search_queries,
                # grounding_metadata ì œê±° - JSON ì§ë ¬í™” ë¶ˆê°€ëŠ¥í•œ ê°ì²´
                'source_count': len(sources)
            }

            return result

        except Exception as e:
            logger.error(f"âŒ Search failed: {e}")

            if self.progress_callback:
                self.progress_callback({
                    'agent': 'search',
                    'type': 'error',
                    'message': f'âŒ ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}'
                })

            return {
                'query': query,
                'summary': f"ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}",
                'sources': [],
                'search_queries': [],
                'source_count': 0,
                'error': str(e)
            }

    def _build_search_prompt(self, query: str, language: str) -> str:
        """
        Build optimized search prompt

        Args:
            query: User's search query
            language: Target language

        Returns:
            Optimized prompt for search grounding
        """
        # Get current date for context
        from datetime import datetime
        current_date = datetime.now().strftime("%Yë…„ %mì›”")
        current_year = datetime.now().year

        if language == "ko":
            prompt = f"""ë‹¤ìŒ ì£¼ì œì— ëŒ€í•´ ì›¹ ê²€ìƒ‰ì„ ìˆ˜í–‰í•˜ê³  ìš”ì•½í•´ì£¼ì„¸ìš”:

ê²€ìƒ‰ì–´: {query}

**í˜„ì¬ ë‚ ì§œ**: {current_date}

ìš”êµ¬ì‚¬í•­:
1. ìµœì‹  ì •ë³´ë¥¼ ìš°ì„  ê²€ìƒ‰ ({current_year}ë…„ ê¸°ì¤€)
2. ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì¶œì²˜ ìš°ì„ 
3. í•œêµ­ì–´ ê²°ê³¼ ìš°ì„  (ìˆëŠ” ê²½ìš°)
4. í•µì‹¬ ì •ë³´ë¥¼ ìì—°ìŠ¤ëŸ¬ìš´ ë¬¸ì¥ìœ¼ë¡œ 3-5ë¬¸ì¥ ìš”ì•½
5. ë¶ˆí•„ìš”í•œ ë§ˆí¬ë‹¤ìš´ í—¤ë”(##, ###)ë‚˜ ì½”ë“œ ë¸”ë¡(```)ì„ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”

ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´ë¡œ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ìš”ì•½í•´ì£¼ì„¸ìš”. ì˜ˆì‹œ:
"{current_year}ë…„ í˜„ì¬ í•œêµ­ì˜ ëŒ€í‘œì ì¸ ë·°í‹° ìœ íŠœë²„ë¡œëŠ” í¬ë‹ˆ, ì´ì‚¬ë°°, ë‹¤ì†œ ë“±ì´ ìˆìŠµë‹ˆë‹¤. í¬ë‹ˆëŠ” êµ¬ë…ì 500ë§Œ ëª…ìœ¼ë¡œ ê°€ì¥ ë§ì€ íŒ”ë¡œì›Œë¥¼ ë³´ìœ í•˜ê³  ìˆìœ¼ë©°, ë©”ì´í¬ì—… íŠœí† ë¦¬ì–¼ê³¼ ì œí’ˆ ë¦¬ë·°ë¥¼ ì£¼ë¡œ ë‹¤ë£¹ë‹ˆë‹¤. ì´ì‚¬ë°°ëŠ” ëŒ€ë‹´í•œ ë©”ì´í¬ì—… ìŠ¤íƒ€ì¼ë¡œ ìœ ëª…í•˜ë©°, ìµœê·¼ì—ëŠ” íŒ¨ì…˜ ì½˜í…ì¸ ë„ í•¨ê»˜ ì œì‘í•˜ê³  ìˆìŠµë‹ˆë‹¤."
"""
        else:
            prompt = f"""Search and summarize information about: {query}

**Current date**: {datetime.now().strftime("%B %Y")}

Requirements:
1. Prioritize recent information ({current_year} based)
2. Use reliable sources
3. Summarize key information in natural sentences (3-5 sentences)
4. Do NOT use markdown headers (##, ###) or code blocks (```)

Provide a natural language summary. Example:
"As of {current_year}, the top Korean beauty YouTubers include Pony, RISABAE, and DaSom. Pony has the largest following with 5 million subscribers, focusing on makeup tutorials and product reviews. RISABAE is known for bold makeup styles and has recently expanded into fashion content."
"""

        return prompt

    def _extract_sources(self, grounding_metadata) -> List[Dict[str, str]]:
        """
        Extract source URLs and titles from grounding metadata

        Args:
            grounding_metadata: Grounding metadata from Gemini response

        Returns:
            List of dicts with 'title' and 'url' keys
        """
        sources = []

        if not grounding_metadata:
            return sources

        try:
            # Extract grounding chunks (web sources)
            if hasattr(grounding_metadata, 'grounding_chunks'):
                for chunk in grounding_metadata.grounding_chunks:
                    if hasattr(chunk, 'web'):
                        web = chunk.web
                        source = {
                            'title': getattr(web, 'title', 'Untitled'),
                            'url': getattr(web, 'uri', '')
                        }
                        if source['url']:
                            sources.append(source)

            # Remove duplicates while preserving order
            seen_urls = set()
            unique_sources = []
            for source in sources:
                if source['url'] not in seen_urls:
                    seen_urls.add(source['url'])
                    unique_sources.append(source)

            logger.info(f"ğŸ“š Extracted {len(unique_sources)} unique sources")

            return unique_sources

        except Exception as e:
            logger.error(f"âŒ Failed to extract sources: {e}")
            return []

    def search_and_extract_urls(self, query: str) -> List[str]:
        """
        Quick search to extract only URLs (for orchestrator)

        Args:
            query: Search query

        Returns:
            List of URLs
        """
        result = self.search(query)
        return [source['url'] for source in result.get('sources', [])]

    def multi_search(self, queries: List[str]) -> Dict[str, Any]:
        """
        Perform multiple searches and combine results

        Args:
            queries: List of search queries

        Returns:
            Combined search results
        """
        all_results = []
        all_sources = []

        for query in queries:
            result = self.search(query)
            all_results.append(result)
            all_sources.extend(result.get('sources', []))

        # Deduplicate sources
        seen_urls = set()
        unique_sources = []
        for source in all_sources:
            if source['url'] not in seen_urls:
                seen_urls.add(source['url'])
                unique_sources.append(source)

        return {
            'queries': queries,
            'results': all_results,
            'combined_sources': unique_sources,
            'total_sources': len(unique_sources)
        }


# Test function
if __name__ == "__main__":
    """Test SearchAgent"""

    # Test 1: Simple search
    agent = SearchAgent()
    result = agent.search("í•œêµ­ ìœ ëª… ë·°í‹° ìœ íŠœë²„ 2025")

    print("\n" + "="*60)
    print("SEARCH RESULTS")
    print("="*60)
    print(f"\nQuery: {result['query']}")
    print(f"\nSummary:\n{result['summary']}")
    print(f"\nSources ({result['source_count']}):")
    for i, source in enumerate(result['sources'][:5], 1):
        print(f"{i}. {source['title']}")
        print(f"   {source['url']}")

    # Test 2: Multiple searches
    print("\n" + "="*60)
    print("MULTI-SEARCH TEST")
    print("="*60)

    multi_result = agent.multi_search([
        "ì‚¼ì„±ì „ì ì£¼ê°€ 2025",
        "AI íŠ¸ë Œë“œ 2025"
    ])

    print(f"\nTotal unique sources: {multi_result['total_sources']}")
